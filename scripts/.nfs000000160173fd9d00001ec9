import os, sys, argparse
import numpy as np
from skimage import io
from scipy.linalg import norm
from scipy import sum, average
from keras.models import Model
from keras.layers import Convolution2D, MaxPooling2D, Lambda
from keras.layers import Input, Concatenate, UpSampling2D, Dropout, BatchNormalization
from keras.callbacks import ModelCheckpoint
from keras import backend as K
from keras.optimizers import Adam
from keras import metrics
from keras.losses import binary_crossentropy
from tch_helpers.tch_make_patches import *

####################
# Helper Functions #
####################

SplitChannels = lambda tch_input: [Lambda(lambda x: K.expand_dims(x[:,:,:,0], -1))(tch_input), 
                   Lambda(lambda x: K.expand_dims(x[:,:,:,1], -1))(tch_input),
                   Lambda(lambda x: K.expand_dims(x[:,:,:,2], -1))(tch_input)]

#####################################
# 3-Output Binary Crossentropy Loss #
#####################################

def bin_crossentropy(y_true, y_pred):
  y_true_0, y_true_1, y_true_2 = SplitChannels(y_true)
  y_pred_0, y_pred_1, y_pred_2 = SplitChannels(y_pred)
  return 0.3*binary_crossentropy(y_true_0, y_pred_0) + 0.6*binary_crossentropy(y_true_1, y_pred_1) + 0.1*binary_crossentropy(y_true_2, y_pred_2)

################################
# Define the Unet Architecture #
################################

def get_unet(patch_height, patch_width, in_ch, out_ch):
  inputs = Input((patch_height, patch_width, in_ch))
  
  conv1 = Convolution2D(32, (3, 3), activation='relu', padding='same')(inputs)
  conv1 = BatchNormalization()(conv1)
  pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)

  conv2 = Convolution2D(64, (3, 3), activation='relu', padding='same')(pool1)
  conv2 = BatchNormalization()(conv2)
  pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)

  conv3 = Convolution2D(128, (3, 3), activation='relu', padding='same')(pool2)
  conv3 = BatchNormalization()(conv3)

  up1 = Concatenate(axis=-1)([UpSampling2D(size=(2, 2))(conv3), conv2])
  conv4 = Convolution2D(64, (3, 3), activation='relu', padding='same')(up1)
  conv4 = BatchNormalization()(conv4)

  up2 = Concatenate(axis=-1)([UpSampling2D(size=(2, 2))(conv4), conv1])
  conv5 = Convolution2D(32, (3, 3), activation='relu', padding='same')(up2)
  conv5 = BatchNormalization()(conv5)

  conv7 = Convolution2D(16, (1, 1), activation='relu')(conv5)
  conv7 = BatchNormalization()(conv7)
  conv7 = Dropout(0.33)(conv7)
  conv7 = Convolution2D(out_ch, (1, 1), activation='sigmoid')(conv7)
  
  model = Model(input=inputs, output=conv7)
  model.compile(optimizer=Adam(lr=1e-4), loss = bin_crossentropy, metrics=['accuracy'])
  return model

#######################
# Parse CMD Line Args #
#######################

def get_args(valid_modes):
  parser = argparse.ArgumentParser(description='VIMSLab Multi-Channel Unet Driver')
  ###
  parser.add_argument("mode", type=str, choices=valid_modes,
                      help="Enter a run mode for the script")
  ###
  parser.add_argument("data_path", type=str, 
                      help="Path to training or prediction data folder")
  ###
  parser.add_argument("-e", "--epochs", type=int, dest='epochs', default=25,
                      help="Number of total epochs to run")
  ###
  parser.add_argument("-b", "--batch-size", type=int, dest='batch_size', default=64, 
                      help="Size of batches to use")
  ###
  parser.add_argument("-o", "--output-path", type=str, dest='output_path', default="./output",
                      help="Path to save segmented images")
  ###
  parser.add_argument("-c", "--checkpoint-path", type=str, dest='checkpoint_path', default="./tch_default_weights.h5",
                      help="Path to checkpointed weights")
  ###
  parser.add_argument("-n", "--norm-path", type=str, dest='norm_parameter_path', default="./tch_default_norm_params.txt",
                      help="Path to normalization parameters")
  ###
  return parser.parse_args()

######################
# Main Control Logic #
######################

if __name__ == "__main__" or True:
  print "Running Two Channel Segmentation..."
  # Parse Arguments
  valid_modes = ['train', 'predict']
  args = get_args(valid_modes=valid_modes)
  mode = args.mode
  data_path = args.data_path
  checkpoint_path = args.checkpoint_path
  output_path = args.output_path
  norm_parameter_path = args.norm_parameter_path
  epochs = args.epochs
  batch_size = args.batch_size
  
  if mode == 'train':
    # Define Model and Model Checkpoint Callback
    model = get_unet(128, 128, in_ch=2, out_ch=3)
    ckpt = ModelCheckpoint(filepath=checkpoint_path, verbose=2, monitor='val_loss', mode='auto', save_best_only=True)
    
    # Build Generators
    imgs_train, masks_train = get_my_patches(data_path, extension='*.tiff', recursive=True)
    n_s = np.arange(imgs_train.shape[0])
    np.random.shuffle(n_s)
    imgs_train = imgs_train[n_s]
    masks_train = masks_train[n_s]

    # Normalize Images
    mean0 = np.mean(imgs_train[:,:,:,0])
    std0 = np.std(imgs_train[:,:,:,0])
    mean1 = np.mean(imgs_train[:,:,:,1])
    std1 = np.std(imgs_train[:,:,:,1])
    imgs_train[:,:,:,0] -= mean0
    imgs_train[:,:,:,0] /= std0
    imgs_train[:,:,:,1] -= mean1
    imgs_train[:,:,:,1] /= std1

    # Write Normalization Parameters if the default path wasn't given
    if norm_parameter_path != "./tch_norm_params.txt":
      with open(norm_parameter_path, 'w') as fp:
        fp.write(','.join(str(elem) for elem in [mean0, std0, mean1, std1]))
    
    # Train the Model
    model.fit(imgs_train, masks_train, batch_size=batch_size, epochs=epochs, verbose=1, shuffle=True, validation_split=0.2, callbacks=[ckpt])
    
  elif mode == 'predict':
    print "Predicting on " + data_path
    # Get Image Paths
    img_paths  = get_images_for_prediction(data_path, extension='*.tiff', recursive=False)

    # Load Normalization Parameters
    print "Loading normalization params..."
    with open(norm_parameter_path, 'r') as fp:
      norm_params = fp.readline()
    mean0, std0, mean1, std1 = [float(elem) for elem in norm_params.split(',')]
    
    print "Number of images: " + str(len(img_paths))
    for i, img_path in enumerate(img_paths):
      print "Working on:", img_path
      
      # Get Image Basename
      img_basename = os.path.splitext(os.path.basename(img_path))[0]
      
      # Load & Normalize Image
      img = io.imread(img_path).astype('float32')
      img = img / 255.
      
      # Normalize Images
      og_img = np.array(img)
      img = img[:,:,0:2]
      img[:,:,0] -= mean0
      img[:,:,0] /= std0
      img[:,:,1] -= mean1
      img[:,:,1] /= std1
      height = img.shape[0]
      width = img.shape[1]

      print '---', 'Image Shape:', img.shape
      print '---', 'Channel 1 Min & Max:', np.min(img[:,:,0]), np.max(img[:,:,0])
      print '---', 'Channel 2 Min & Max:', np.min(img[:,:,1]), np.max(img[:,:,1])
      
      # Create Batch Dimension, Create Network, Load Weights, and Predict on Image
      img = np.expand_dims(img, axis=0)
      model = get_unet(height,width,in_ch=2, out_ch=3)
      model.load_weights(checkpoint_path)
      out_img = model.predict(img)
      
      # Construct Output Image & Save
      im = np.zeros((img.shape[1], img.shape[2], 3))
      im[:,:,0:3] = 1.*np.squeeze(out_img)
      
      print '---','Output Image Shape:', im.shape
      print '---', 'Output Channel 1 Min & Max:', np.min(im[:,:,0]), np.max(im[:,:,0])
      print '---', 'Output Channel 2 Min & Max:', np.min(im[:,:,1]), np.max(im[:,:,1])
      print '---', 'Output Channel 3 Min & Max:', np.min(im[:,:,2]), np.max(im[:,:,2])
      
      outpath = os.path.join(output_path,  img_basename + '_predicted.png').replace(' ','_')
      print "Saving to", outpath
      io.imsave(outpath, im)
      
  